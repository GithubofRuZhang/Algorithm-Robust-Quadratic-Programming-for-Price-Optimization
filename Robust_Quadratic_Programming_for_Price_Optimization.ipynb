{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM69x9pBWbZwCg+xd90w9rT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GithubofRuZhang/Algorithm-Robust-Quadratic-Programming-for-Price-Optimization/blob/main/Robust_Quadratic_Programming_for_Price_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\begin{array}{l}\\qquad g(x, \\gamma):=v(x)^{\\top}\\left(\\hat{Q}+\\lambda \\frac{\\gamma M_{1}+M_{2} / \\gamma}{2}\\right) v(x) . \\\\ \\text { where } \\\\ \\qquad M_{1}:=L_{1}^{\\top} L_{1}, \\quad M_{2}:=L_{2}^{\\top} L_{2} .\\end{array} $"
      ],
      "metadata": {
        "id": "uF3TgJCTQdOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function $ h:(0, \\infty) \\rightarrow \\mathbf{R} $ by\n",
        "$$\n",
        "h(\\gamma):=\\min _{x \\in \\mathcal{X}} g(x, \\gamma)\n",
        "$$"
      ],
      "metadata": {
        "id": "ahIhniOVQk5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm 1 Golden Section Search\n",
        "\n",
        "Require: $ \\hat{Q}, L_{1}, L_{2}, \\lambda, \\alpha, \\beta, \\delta $\n",
        "\n",
        "Initialize $ a=\\alpha, b=\\beta, r=(\\sqrt{5}-1) / 2 $\n",
        "\n",
        "while $ |a-b| \\geq \\delta $ do\n",
        "\n",
        "$ c \\leftarrow b-r *(b-a), d \\leftarrow a+r *(b-a) $\n",
        "\n",
        "$ b \\leftarrow d $ if $ h(c)<h(d) $, and $ a \\leftarrow c $ otherwise.\n",
        "end while\n",
        "\n",
        "Output $ \\tilde{x}:= oracle(x, \\tilde{\\gamma})=\\arg \\min _{x \\in \\mathcal{X}} g(x, \\tilde{\\gamma}) $ where $ \\tilde{\\gamma}=(a+b) / 2 $."
      ],
      "metadata": {
        "id": "vS5FMB_hQSnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-Ub3R4sbQctl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 假设参数和函数\n",
        "# 这里仅为示例，具体实现需要根据问题的实际参数和约束来定义\n",
        "\n",
        "# 示例参数\n",
        "M = 3  # 假设有3个产品，可以根据需要调整\n",
        "Q_hat = np.random.rand(M, M)# 假设的 Q_hat\n",
        "L1 = np.random.rand(M, M) # 假设的 L1\n",
        "L2 = np.random.rand(M, M)  # 假设的 L2\n",
        "lambda_ = 1  # 假设的 lambda\n",
        "alpha = 0.1  # 初始搜索区间下限\n",
        "beta = 2  # 初始搜索区间上限\n",
        "delta = 0.001  # 精度要求\n",
        "\n",
        "# 定义 g(x, gamma) 函数\n",
        "def g(x, gamma):\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    return x.T @ term @ x\n",
        "\n",
        "# 定义 h(gamma) 函数，这里简化为使用固定的 x 值\n",
        "def h(gamma):\n",
        "    x = np.array([1] * M)  # 假设的 x 值，实际情况中应该是优化问题的解\n",
        "    return g(x, gamma)\n",
        "\n",
        "# 黄金分割搜索算法\n",
        "def golden_section_search(alpha, beta, delta):\n",
        "    r = (np.sqrt(5) - 1) / 2\n",
        "    a, b = alpha, beta\n",
        "\n",
        "    while abs(b - a) >= delta:\n",
        "        c = b - r * (b - a)\n",
        "        d = a + r * (b - a)\n",
        "        if h(c) < h(d):\n",
        "            b = d\n",
        "        else:\n",
        "            a = c\n",
        "\n",
        "    return (a + b) / 2\n",
        "\n",
        "# 执行算法\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "gamma_tilde\n"
      ],
      "metadata": {
        "id": "nL-pScKzbkZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ccc8c7-850c-4b43-c8f1-c70d993d91fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6407877624044351"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CZZCOes-QRgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 更新算法以支持任意维度的x\n",
        "# 假设参数和函数更新\n",
        "M = 3  # 假设有3个产品，可以根据需要调整\n",
        "Q_hat = np.random.rand(M, M)# 假设的 Q_hat\n",
        "L1 = np.random.rand(M, M) # 假设的 L1\n",
        "L2 = np.random.rand(M, M)  # 假设的 L2\n",
        "lambda_ = 1  # 假设的 lambda\n",
        "alpha = 0.1  # 初始搜索区间下限\n",
        "beta = 2  # 初始搜索区间上限\n",
        "delta = 0.001  # 精度要求\n",
        "x_options = np.array([0.6, 0.7, 0.8, 0.9, 1.0])  # 简化的 x 选择\n",
        "\n",
        "x_options = np.array([0.6, 0.7, 0.8, 0.9, 1.0])  # 简化的 x 选择\n",
        "\n",
        "# 更新 Oracle 算法以支持 M 维 x\n",
        "def oracle(gamma, M, x_options):\n",
        "    min_val = np.inf\n",
        "    x_opt = None\n",
        "\n",
        "    # 生成所有可能的x组合\n",
        "    X = np.array(np.meshgrid(*[x_options for _ in range(M)])).T.reshape(-1, M)\n",
        "\n",
        "    # 遍历所有x组合\n",
        "    for x in X:\n",
        "        val = g(x, gamma)\n",
        "        if val < min_val:\n",
        "            min_val = val\n",
        "            x_opt = x\n",
        "\n",
        "    return x_opt\n",
        "\n",
        "# 使用更新的 Oracle 算法执行黄金分割搜索\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "x_tilde = oracle(gamma_tilde, M, x_options)\n",
        "\n",
        "gamma_tilde, x_tilde\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlUcqv1-C_9L",
        "outputId": "b2b475b5-b366-42d7-a48c-4ead8bf80c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7817463680623973, array([0.6, 0.6, 0.6]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 更新的参数定义\n",
        "# 定义假设的参数\n",
        "M = 3  # 维度\n",
        "Q_hat = np.random.rand(M, M)# 假设的 Q_hat\n",
        "L1 = np.random.rand(M, M) # 假设的 L1\n",
        "L2 = np.random.rand(M, M)  # 假设的 L2\n",
        "lambda_ = 1  # 假设的 lambda\n",
        "lambda_ = 1\n",
        "alpha = 0.1\n",
        "beta = 2\n",
        "delta = 0.001\n",
        "\n",
        "\n",
        "# 重新定义g(x, gamma)以匹配更新的参数\n",
        "def g(x, gamma):\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    # print(M1)\n",
        "    # print(M2)\n",
        "    # print(term)\n",
        "    # print(x)\n",
        "    return x.T @ term @ x\n",
        "\n",
        "# 其他函数保持不变\n",
        "\n",
        "# 执行黄金分割搜索并使用Oracle算法\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "x_tilde = oracle(gamma_tilde, M, x_options)\n",
        "\n",
        "gamma_tilde, x_tilde\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1q-prpOGvgE",
        "outputId": "80dff69d-50cd-4a0f-a586-735424133b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.5760360673325113, array([0.6, 0.6, 0.6]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "要解决给定的 $ g(x, \\gamma) $ 函数在连续区间上的优化问题 $ \\tilde{x}:=\\arg \\min _{x \\in \\mathcal{X}} g(x, \\tilde{\\gamma}) $ ，我们可以使用数值优化方法。对于这个问题，一个简单但有效的方法是使用梯度下降法，尽管需要计算 $ g(x, \\gamma) $ 关于 $ x $ 的梯度。\n",
        "\n",
        "假设 $ x $ 是一个 $ M $ 维向量，并且 $ g(x, \\gamma) $ 对每个元素 $ x_{i} $ 的梯度可以计算。在实际情况中，如果 $ g(x, \\gamma) $ 是一个光滑函数，我们可以通过求导来获得这些梯度。\n",
        "\n",
        "简化的梯度下降法\n",
        "梯度下降法是一种迭代算法，通过在每一步中沿着目标函数的负梯度方向更新变量来寻找最小值。对于 $ g(x, \\gamma) $ ，更新公式可以写为:\n",
        "$$\n",
        "x^{(k+1)}=x^{(k)}-\\alpha \\nabla_{x} g\\left(x^{(k)}, \\gamma\\right)\n",
        "$$\n",
        "\n",
        "其中， $ x^{(k)} $ 是第 $ k $ 步的 $ x $ 值， $ \\alpha $ 是学习率，一个小的正数， $ \\nabla_{x} g\\left(x^{(k)}, \\gamma\\right) $ 是 $ g(x, \\gamma) $在 $ x^{(k)} $ 处对 $ x $ 的梯度。\n",
        "\n",
        "算法步骤\n",
        "1. 初始化: 选择一个初始点 $ x^{(0)} $ ，设置学习率 $ \\alpha $ 和容忍度 $ \\epsilon $ 。\n",
        "2. 迭代更新:\n",
        "- 计算梯度: $ \\nabla_{x} g\\left(x^{(k)}, \\tilde{\\gamma}\\right) $ 。\n",
        "- 更新 $ x: x^{(k+1)}=x^{(k)}-\\alpha \\nabla_{x} g\\left(x^{(k)}, \\tilde{\\gamma}\\right) $ 。\n",
        "3. 终止条件: 当 $ \\left\\|x^{(k+1)}-x^{(k)}\\right\\|<\\epsilon $ 时停止迭代。\n",
        "\n",
        "示例实现\n",
        "下面是一个简化的示例，展示如何实现这一过程。请注意，我们需要根据 $ g(x, \\gamma) $ 的具体形式来计算梯度。"
      ],
      "metadata": {
        "id": "QG7Efo4pLBCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_g(x, gamma):\n",
        "    # 这里需要根据g(x, gamma)的具体形式来计算梯度\n",
        "    # 示例梯度计算，仅供参考\n",
        "    grad = 2 * (Q_hat + lambda_ * (gamma * L1.T @ L1 + (L2.T @ L2) / gamma) / 2) @ x\n",
        "    return grad\n",
        "\n",
        "def gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient_g(x, gamma)\n",
        "        x_new = x - alpha * grad\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "    return x\n",
        "\n",
        "# 使用示例\n",
        "\n",
        "# 生成在指定范围内的随机数，例如在[0.6, 1.0]区间内\n",
        "x_init = np.random.rand(M) * 0.2 + 0.8\n",
        "gamma = gamma_tilde  # 之前计算得到的gamma_tilde\n",
        "alpha = 0.0000001  # 学习率\n",
        "epsilon = 1e-3  # 容忍度\n",
        "\n",
        "x_opt = gradient_descent(x_init, gamma, alpha, epsilon)\n",
        "x_opt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVWuhIDGDPn8",
        "outputId": "fee9993b-e7c4-4dfa-f3f1-fe2c7b3020cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.92965874, 0.99867665, 0.98175512])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义梯度计算函数\n",
        "def gradient_g(x, gamma):\n",
        "    # 根据g(x, gamma)的实际表达式计算梯度\n",
        "    # 这里使用示例中的假设参数进行计算\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    grad = 2 * (term.T+term) @ x\n",
        "    return grad\n",
        "\n",
        "# 定义梯度下降法\n",
        "def gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient_g(x, gamma)\n",
        "        x_new = x - alpha * grad\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "    return x\n",
        "\n",
        "# 初始化参数\n",
        "x_init = np.random.rand(M)*60+100  # 随机初始化x\n",
        "print(x_init)\n",
        "gamma = 0.9997  # 使用之前计算得到的gamma_tilde\n",
        "alpha = 0.0000001  # 学习率\n",
        "epsilon = 1e-6  # 容忍度\n",
        "\n",
        "# 执行梯度下降算法找到最优x\n",
        "x_opt = gradient_descent(x_init, gamma, alpha, epsilon)\n",
        "\n",
        "x_opt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKQryR5tFEqz",
        "outputId": "a9ce7307-72e4-4ea8-ed16-bc1edd9fca5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120.5522323  133.76145453 115.88353019]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([120.36190874, 133.60610404, 115.71042473])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "要将梯度下降算法改造为使用自适应学习率，我们可以采用一些流行的优化算法中的技术，如 Adam（Adaptive Moment Estimation）或 RMSprop（Root Mean Square Propagation）。这些算法通过调整每个参数的学习率来改善收敛速度和稳定性，特别是在复杂的优化问题中。\n",
        "\n",
        "这里，我将展示如何将原先的梯度下降算法修改为使用一个简化版本的 Adam 算法。Adam 算法结合了动量（Momentum）和自适应学习率的概念，对于每个参数独立地调整学习率。"
      ],
      "metadata": {
        "id": "lHX4tIflK09O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个 'adam_gradient_descent' 函数接收相同的参数：初始 ' $ x $ '、'gamma 、学习率 'alpha'、容忍度 'epsilon “以及最大迭代次数 'max_iter'。它使用 Adam 算法的核心概念，但省略了一些复杂性以保持示例的清晰性。\n",
        "- $ m^{\\prime} $ 和 $ v^{\\prime} $ 分别存储关于梯度的一阶 (平均) 和二阶 (未中心化的方差) 矩估计。\n",
        "- 'beta1 '和 'beta2'控制这些矩估计的指数衰减率, 这是 Adam 算法特有的超参数。\n",
        "- ' $ m_{-} $hat' 和 ' $ v $ _ hat' 是对 ' $ m $ ' 和 ' $ v $ ' 的偏差校正，用于在算法的早期阶段调整这些估计。"
      ],
      "metadata": {
        "id": "XHZ-x-VzLYTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 定义假设的参数\n",
        "M = 10  # 维度，表示问题的规模或变量的数量\n",
        "Q_hat = np.random.rand(M, M)  # 随机生成假设的 Q_hat，模拟真实情况下未知的参数矩阵\n",
        "L1 = np.random.rand(M, M)  # 随机生成假设的 L1，代表某种线性变换或约束\n",
        "L2 = np.random.rand(M, M)  # 随机生成假设的 L2，同样代表某种线性变换或约束\n",
        "lambda_ = 1  # 假设的 lambda，用于调节正则化项的强度\n",
        "\n",
        "# 定义 g(x, gamma) 函数的梯度\n",
        "def gradient_g(x, gamma):\n",
        "    M1 = L1.T @ L1  # 计算L1的自相关矩阵\n",
        "    M2 = L2.T @ L2  # 计算L2的自相关矩阵\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2  # 综合所有项，形成目标函数的系数矩阵\n",
        "    grad = 2 * (term.T + term) @ x  # 计算目标函数关于x的梯度\n",
        "    return grad\n",
        "\n",
        "# 定义 Adam 梯度下降算法\n",
        "def adam_gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init  # 初始化x\n",
        "    m = np.zeros(x.shape)  # 初始化一阶动量估计为0\n",
        "    v = np.zeros(x.shape)  # 初始化二阶动量估计为0\n",
        "    beta1 = 0.9  # 动量衰减率，用于一阶估计\n",
        "    beta2 = 0.999  # 动量衰减率，用于二阶估计\n",
        "    eta = alpha  # 学习率\n",
        "    delta = 1e-8  # 用于避免除以零的小常数\n",
        "\n",
        "    for i in range(1, max_iter + 1):\n",
        "        grad = gradient_g(x, gamma)  # 计算当前x的梯度\n",
        "\n",
        "        # 更新一阶和二阶动量估计\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "\n",
        "        # 对动量估计进行偏差校正\n",
        "        m_hat = m / (1 - beta1 ** i)\n",
        "        v_hat = v / (1 - beta2 ** i)\n",
        "\n",
        "        # 更新参数x\n",
        "        x_new = x - eta * m_hat / (np.sqrt(v_hat) + delta)\n",
        "\n",
        "        # 检查是否满足停止准则\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "\n",
        "    return x\n",
        "\n",
        "# 初始化参数并执行算法\n",
        "x_init = np.random.rand(M) * 0.4 + 0.6  # 随机初始化x，确保其初始值在[0.6, 1.0]之间\n",
        "gamma = 0.9997  # 给定的gamma值\n",
        "alpha = 0.0001  # 学习率，较小值以确保稳定收敛\n",
        "epsilon = 1e-6  # 收敛阈值，当连续两次迭代的差距小于此值时停止迭代\n",
        "max_iter = 1000  # 最大迭代次数\n",
        "\n",
        "# 使用Adam算法优化x\n",
        "x_opt_adam = adam_gradient_descent(x_init, gamma, alpha, epsilon, max_iter)\n",
        "\n",
        "x_opt_adam\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSy_2vlTMZEJ",
        "outputId": "314f8aa7-6b61-4718-c0cc-28ed9587b284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.54606265, 0.78306905, 0.70148065, 0.66228437, 0.68859742,\n",
              "       0.59015949, 0.53485485, 0.71166606, 0.89515562, 0.86688419])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYtv4NVdNdQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}